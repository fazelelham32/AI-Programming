The Pac-Man Projects
Logical agent design:
What does this pacman understand about the environment?
- He sees the place of souls. - He sees the location of the food. - Where is he?
- Where is the obstacle, energy capsules
The operation it performs moves in 4 directions.
He should be able to eat all the food with minimal movements
One point is deducted for each move he makes.

Overview
The Pac-Man projects were developed for CS 188. They apply an array of AI techniques to playing Pac-Man. However, these projects don't focus on building AI for video games. Instead, they teach foundational AI concepts, such as informed state-space search, probabilistic inference, and reinforcement learning. These concepts underly real-world application areas such as natural language processing, computer vision, and robotics.
We designed these projects with three goals in mind. The projects allow you to visualize the results of the techniques you implement. They also contain code examples and clear directions, but do not force you to wade through undue amounts of scaffolding. Finally, Pac-Man provides a challenging problem environment that demands creative solutions; real-world AI problems are challenging, and Pac-Man is too. 

Projects Overview 
P0: UNIX/Python Tutorial 
This short UNIX/Python tutorial introduces students to the Python programming language and the UNIX environment.

P1: Search 
Students implement depth-first, breadth-first, uniform cost, and A* search algorithms. These algorithms are used to solve navigation and traveling salesman problems in the Pacman world. 

Mini-Contest 1: Multi-Agent Pacman 
Students will apply the search algorithms and problems implemented in Project 1 to handle more difficult scenarios that include controlling multiple pacman agents and planning under time constraints 

P2: Multi-Agent Search 
Classic Pacman is modeled as both an adversarial and a stochastic search problem. Students implement multiagent minimax and expectimax algorithms, as well as designing evaluation functions.

Mini-Contest 2: Multi-Agent Adversarial Pacman 
This minicontest involves a multiplayer capture-the-flag variant of Pacman, where agents control both Pacman and ghosts in coordinated team-based strategies. Each team will try to eat the food on the far side of the map, while defending the food on their home side.

P3: Reinforcement Learning 
Students implement model-based and model-free reinforcement learning algorithms, applied to the AIMA textbook's Gridworld, Pacman, and a simulated crawling robot. 

P4: Ghostbusters 
Probabilistic inference in a hidden Markov model tracks the movement of hidden ghosts in the Pacman world. Students implement exact inference using the forward algorithm and approximate inference via particle filters. 

P5: Machine Learning 
Students implement the perceptron algorithm and neural network models, and apply the models to several tasks including digit classification. 

Contest: Pacman Capture the Flag 
Students create strategies for a team of two agents to play a multi-player capture-the-flag variant of Pacman. 

Technical Notes
The Pac-Man projects are written in pure Python 3.6 and do not depend on any packages external to a standard Python distribution. 

Support
This project was supported by the National Science foundation under CAREER grant 0643742. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation (NSF). 

Credits
The projects were developed by John DeNero, Dan Klein, Pieter Abbeel, and many others. 

Why python for AI?
Because it has support for pre-built libraries that python is very convenient for AI developers because all of the algorithms machine learning algorithms and deep learning algorithms are already predefined in libraries right so you don’t have to actually sit down and code eacd and every algorithm that will take a lot of time and that’s a very time-consuming task 

Introduction to AI for video games:
Data scientists will hair humans to hand label their data sets using services like Amazon’s Mechanical Turk, but ideally we don’t need labels we can just train our algorithms on unlabelled data since the vast majority of the world’s data does not in fact have labels so if we want to train unsupervised meaning to labels, then we can use techniques like clustering and anomaly detection these are fast improving but there’s also room for another class of learning techniques that are based on trial and error in an environment setting this is called reinforcement learning the basic idea is that in RL the labels are time delayed and instead of calling them labels we call them rewards while supervised learning tells you have to achieve your goal RL tells you how well you achieved the goal there are lots of problem settings where the idea of?
Time delayed labels makes more sense think about if you were tasked with creating an AI that learns how best to  control the temperature in a data centre. How are you going to tell your algorithm?
Robotics has been a testbed for RL research for decades there have been several successes in getting RL agents to learn to play sports navigate a helicopter autonomously gain robots to walk and getting them to full of laundry
We’ve created some seriously capable robots that are theoretically able to do any task a healthy human could do but the reason they’re still so limited is because of software robotics is a software problem not a hardware problem in parallel to the robotics world game environments have also been a testbed for RL since they are safer than the real world and the barrier to entry is just having  a laptop so anyone can test our their algorithms two of the most popular AL research institutions in the world open AI and deep mind extensively  use game environments to train and test their algorithms and their world class. 
The simple game of tic-tac-toe 
Where the goal is to be the first to successfully create three in a row and we have our AI which we’ll call an agent our goal is to have this AI learn how to become really good at playing tic-tac-toe against humans rather than  just hard coding in a bunch of if-then statements how can we formalize this problem?


AI in Medicine | Medical Imaging Classification (TensorFlow Tutorial)
How they approach patient problems and the type of health system that supports them this combination is what causes such wide variations in clinical outcomes and it’s the reason why machine learning is the best solution out there to improve doctors capabilities if you can’t beat them join them there’s so much potential here.
NLP a branch of AI that helps computers understand and interpret human language can review thousands of medical records and output the optimal steps for evaluating and managing patients with many illnesses some doctors are really gifted at what day do like dr. oz just kidding an AI can learn from the best by watching them do their work if all doctors match the performance of the top 20% patient deaths from a variety of diseases would decrease by the hundreds of thousands per year and wild doctors have natural biases AI is more likely to produce objective diagnosis for patients without preconceived socio-economic notions which can produce disparities in care ml will become an essential tool for doctors like the stethoscope has been and as more of the profession is automated human empathy and compassion for patients will become paramount their success in the field also good look let’s just be real so how do we pick a problem to solve the AI and healthcare market is expected to grow at an incredible forty eight percent compound annual growth rate in the next five years according to research and markets the teach giants like Google and Microsoft have massive amount of data talent and computing power they have a huge advantage when it comes to building horizontal products these are products that can be applied to many use cases but as a start-up you can build a vertical product one that tackles a single problem very well since they don’t have the time to do that one way to do this is to find relevant problems in online communities where doctors congregate these can be forums slack channels subreddits Grey’s Anatomy chat rooms about McDreamy and see what kind of problems they’re having, another way would be to call up or schedule a visit to a local practice to hear firsthand about the type of problems they’re having listening before acting is an important first step when finding the right problem to solve, eventually come across a problem that is deal with by multiple doctors let’s say for the case of this demo its diagnosing diabetic retinopathy correctly .
Artificial Curiosity
In any kind of real time environment scenario where time is a relevant data point whether that’s a simulation or a smart power grid or robotic movement we can use a class of algorithms called RL to learn an optimal policy for our agent so that it can achieve a predefined objective that objective could be any number of things transport of power across nodes at the lowest cost detecting intruders in a real-time security system or just winning a game an AI agents policy then is a function that given an environment state will return the optimal action that an agent should take to reach that objective there are several possible functions that our AI agent will need to approximate in order to learn the optimal policy and several known techniques can help it do that but of all of the function approximation techniques neural networks have shown the most promising results thus using neural networks to approximate one or several functions involved in a reinforcement learning scenario is considered DRL and it’s no secret that deep RL is the hottest the subfield of AI right now with many researchers adding to the scope of human knowledge on the topic every single week well of them being the research group that we’re talking about in this  video.
We can the curiosity as the error between the predicted state given our current state and action as well as a new real state in the case of our video game or agent is Mario and each state will be a game frame which consists of a bunch of pixels the problem though is that there are so many pixel values to predict in a single game frame so the entire process will be very computationally expensive they decided that making a prediction in the raw sensory space of game pixels wasn’t the optimal way to frame the problem so they instead transformed the raw sensory input which is an array of pixels into a lower dimensional space that only contains
Relevant information but what do you mean by relevant what makes some data relevant and other data irrelevant to our agent the way they ensured that it only contained relevant information by is defining a rule for what data could be considered relevant beforehand relevant data needs to model both environment objects that can be controlled by the agent as well as environment objects that can affect the agent everything else is not in the agent control and has no effect on them and we van discard that so in our super Mario game our agent Mario would need to model himself since he’s controlled by the agent and as the enemy koopas approach we can’t control them but they can affect our agent so they’re relevant keep your enemies close right but we don’t need to model the cloud in the sky  because it doesn’t affect our agent and we can’t control it 
This way by constraining what data we represent we can develop a feature representation with much less noise our desired embedding space will be compact and informational dense 
Curiosity May Be Vital for Truly Smart AI
A computer algorithm equipped with a form of artificial curiosity can learn to solve tricky problems even when it isn’t immediately clear what actions might help it reach this goal.
Researchers at the University of California, Berkeley, developed an “intrinsic curiosity model” to make their learning algorithm work even when there isn’t a strong feedback signal. The curiosity model developed by this team sees the AI software controlling a virtual agent in a video game seek to maximize its understanding of its environment and especially aspects of that environment that affect it. There have been previous efforts to give AI agents curiosity, but these have tended to work in a more simplistic way.
The trick may help address a shortcoming of today’s most powerful machine-learning techniques, and it could point to ways of making machines better at solving real-world problems.
“Rewards in the real world are very sparse,” says Pulkit Agrawal, a PhD student at UC Berkeley who carried out the research with colleagues. “Babies do all these random experiments, and you can think of that as a kind of curiosity. They are learning some sort of skills.”
Several powerful machine-learning techniques have made machines smarter in recent years. Among these, a method known as reinforcement learning has made it possible for machines to accomplish things that would be difficult to define in code. Reinforcement learning involves using positive rewards to guide an algorithm’s behavior toward a particular goal (see “10 Breakthrough Technologies 2017: Reinforcement Learning”).
Reinforcement learning was a fundamental part of AlphaGo, a program developed by DeepMind, to play the abstract and complex board game Go with incredible skill. The technique is now being explored as a way to imbue machines with other skills that may be impossible to code manually. For instance, it can provide a way for a robot arm to work out for itself how to perform a desired chore.
Reinforcement learning has its limitations, though. Agrawal notes that it often takes a huge amount of training to learn a task, and the process can be difficult if the feedback required isn’t immediately available. For instance, the method doesn’t work for computer games in which the benefits of certain behaviors isn’t immediately obvious. That’s where curiosity could help.
The researchers tried the approach, in combination with reinforcement learning, within two simple video games: Mario Bros., a classic platform game, and VizDoom, a basic 3-D shooter title.
In both games, the use of artificial curiosity made the learning process more efficient. In the 3-D game, for instance, instead of spending an excessive amount of time bumping into walls, the agent moved around its environment, learning to navigate more quickly. Even without any other reward, the agent was able to navigate both games surprisingly well. In Mario Bros. it learned to avoid getting killed because this lessened its ability to explore and learn about its environment.
A paper describing the research will be published at a major AI conference later this year.
Artificial curiosity has been an active area of research for some time. Pierre-Yves Oudeyer, a research director at the French Institute for Research in Computer Science and Automation, has pioneered, over the past several years, the development of computer programs and robots that exhibit simple forms of inquisitiveness.
“What is very exciting right now is that these ideas, which were very much viewed as ‘exotic’ by both mainstream AI and neuroscience researchers, are now becoming a major topic in both AI and neuroscience,” Oudeyer says.
The work could have real practical benefits. The UC Berkeley team is keen to test it on robots that use reinforcement learning to work out how to do things like grasps awkward objects. Agrawal says robots can waste a huge amount of time performing random gestures. When equipped with innate curiosity, such a robot should more quickly explore its surroundings and experiment with nearby objects, he says.
Brenden Lake, a research scientist at New York University who builds computational models of human cognitive capabilities, says the work seems promising. “Developing machines with similar qualities is an important step toward building machines that learn and think like people,” he said in an e-mail. “It’s very impressive that by using only curiosity-driven learning, the agent can learn to navigate a level in Mario. The agent doesn’t even look at the game score.”
At the same time, says Lake, the inquisitiveness demonstrated by the new program is actually pretty different from, say, that of a child. Humans tend to display a much deeper interest in their world, he says.
“It’s a very egocentric form of curiosity,” Lake says. “The agent is only curious about features of its environment that relate to its own actions. People are more broadly curious. People want to learn about the world in ways less directly tied to their own actions.”
Large-Scale Study of
Curiosity-Driven Learning
Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal.

In this paper:
(a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments.
(b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.).
(c) We demonstrate limitations of the prediction-based rewards in stochastic setups.
Curiosity-Driven Learning Without Extrinsic Rewards
A snapshot of the 54 environments investigated in the paper. We show that agents are able to makeprogress using no extrinsic reward, or end-of-episode signal, and only using curiosity. 

In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.

Artificial Intelligence in Healthcare 
Let humans do what they do well, and let machines do what they do well. In the end, we may maximize the potential of both.
The impediment to action advances action… that which is in the way becomes the way.
